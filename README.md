# TickerAI ðŸ“Š

AI-powered stock analysis using local LLM models. Analyze stock tickers with RAG (Retrieval Augmented Generation) - completely free and private.

## Features

- ðŸ¦™ **Local LLM** - Uses Llama via Ollama (no API keys needed)
- ðŸ”’ **100% Private** - Everything runs on your machine
- ðŸ’° **Zero Cost** - No API fees
- ðŸ”Œ **Extensible** - Easy to swap LLM providers
- ðŸ“š **RAG-Powered** - ChromaDB + semantic search
- ðŸŽ¯ **Multiple Formats** - Supports TXT, MD, PDF, JSON documents

## Quick Start

### One-Command Setup

```bash
./setup.sh
```

This automated script will:
1. âœ… Check Python 3.10+ installation
2. âœ… Install Ollama
3. âœ… Download Llama model
4. âœ… Create virtual environment
5. âœ… Install dependencies
6. âœ… Initialize knowledge base

### Manual Setup (if preferred)

```bash
# 1. Install Ollama
brew install ollama  # macOS
# OR
curl -fsSL https://ollama.ai/install.sh | sh  # Linux

# 2. Start Ollama & download model
ollama serve  # Keep running
ollama pull llama3.2

# 3. Setup Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 4. Initialize knowledge base
python main.py setup
```

## Usage

### Interactive Mode

```bash
source venv/bin/activate
python main.py interactive
```

Example:
```
ðŸ“Š Enter ticker and question: AAPL: What are their main products?
ðŸ“Š Enter ticker and question: TSLA: What are the risks?
ðŸ“Š Enter ticker and question: stats
ðŸ“Š Enter ticker and question: quit
```

### Single Query

```bash
python main.py query AAPL "What is Apple's revenue?"
python main.py query TSLA "What are Tesla's competitive advantages?"
```

### Add Your Own Documents

1. Add documents to `knowledge/` directory (TXT, MD, PDF, JSON)
2. Refresh knowledge base:
   ```bash
   python main.py setup --force
   ```

## Configuration

Edit `config.py` or set environment variables:

```bash
# Change LLM model
export LLM_MODEL="llama3.1"  # or llama2, llama3.2:1b, etc.

# Change LLM provider (for future extensibility)
export LLM_PROVIDER="ollama"  # currently only ollama supported

# Adjust generation parameters
export LLM_TEMPERATURE="0.7"
export LLM_MAX_TOKENS="1000"
```

## Architecture

```
User Query â†’ MCP Client â†’ MCP Server â†’ Vector Store (ChromaDB)
                              â†“
                         LLM Provider (Abstracted)
                              â†“
                         Ollama (Llama 3.2)
                              â†“
                          Response
```

### Extensible LLM Design

The application uses an abstraction layer (`llm_provider.py`) that makes it easy to swap LLM providers:

```python
# Currently using Ollama
from llm_provider import OllamaProvider
llm = OllamaProvider(model="llama3.2")

# Future: Switch to OpenAI
from llm_provider import OpenAIProvider
llm = OpenAIProvider(api_key="...", model="gpt-4")

# Or implement your own
class CustomProvider(BaseLLMProvider):
    def generate(self, prompt, ...):
        # Your implementation
        pass
```

## Available Models

| Model | Size | RAM | Best For |
|-------|------|-----|----------|
| llama3.2:1b | 1GB | 4GB | Low-end systems |
| llama3.2 | 2GB | 8GB | **Recommended** |
| llama3.1 | 5GB | 16GB | Better quality |
| llama2 | 4GB | 16GB | Stable option |

Change models:
```bash
ollama pull llama3.1
export LLM_MODEL="llama3.1"
python main.py interactive
```

## Project Structure

```
TickerAI/
â”œâ”€â”€ setup.sh              # One-command setup script
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ config.py             # Configuration
â”œâ”€â”€ llm_provider.py       # LLM abstraction layer
â”œâ”€â”€ mcp_server.py         # MCP server
â”œâ”€â”€ mcp_client.py         # MCP client
â”œâ”€â”€ vector_store.py       # ChromaDB integration
â”œâ”€â”€ document_processor.py # Document processing
â”œâ”€â”€ main.py               # CLI interface
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ knowledge/            # Your documents
    â”œâ”€â”€ AAPL_info.md
    â”œâ”€â”€ TSLA_info.md
    â””â”€â”€ general_stock_analysis.md
```

## Troubleshooting

**Ollama not running:**
```bash
ollama serve
```

**Model not found:**
```bash
ollama pull llama3.2
```

**Import errors:**
```bash
source venv/bin/activate
pip install -r requirements.txt
```

**Slow responses:**
- Use smaller model: `llama3.2:1b`
- Check system resources
- Close other applications

## Extending to Other LLMs

To add a new LLM provider:

1. Edit `llm_provider.py`:
```python
class YourProvider(BaseLLMProvider):
    def generate(self, prompt, system_prompt, temperature, max_tokens):
        # Implement your LLM API call
        return response_text
    
    def get_model_name(self):
        return self.model
```

2. Update factory function:
```python
def get_llm_provider(provider_type):
    if provider_type == "your_provider":
        return YourProvider()
```

3. Set in config:
```bash
export LLM_PROVIDER="your_provider"
```

## Benefits vs Cloud APIs

| Feature | TickerAI (Local) | Cloud APIs |
|---------|------------------|------------|
| Cost | Free | Pay per request |
| Privacy | 100% local | Data sent to server |
| Internet | Not required* | Required |
| Setup | 5 minutes | Instant |
| Quality | Very good | Excellent |

*After initial model download

## Requirements

- Python 3.10+
- 8GB+ RAM (recommended)
- 5GB disk space (for models)
- macOS, Linux, or Windows

## Support

- Check `config.py` for configuration options
- Run `./setup.sh` to reset environment
- Ollama docs: https://ollama.ai
- MCP docs: https://modelcontextprotocol.io

## License

MIT License - Free to use and modify

---

**Built with:** Python | Ollama | ChromaDB | MCP | sentence-transformers
